{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HiteshShettyK90/langchain-apps/blob/main/%5BStudent_Version%5D_Langchain_Week1_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook is part of the course: [LLM Apps with Langchain](https://uplimit.com/course/llm-apps-with-langchain) and is created by Sidharth Ramachandran as the project for Week 1 of the course."
      ],
      "metadata": {
        "id": "h-T0u92GVJE6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Let's build \"PillPal\" - a Telegram bot and website that allows users to ask questions based on the Patient Information Leaflet (PIL) or drug guide that comes with any medication."
      ],
      "metadata": {
        "id": "BmgRqB0z3JLd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Package Information Leaflet](https://i.ibb.co/ZSdGyjV/ima-image-36904.jpg)"
      ],
      "metadata": {
        "id": "zcmYA3M8aGp3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I'm sure you have all come across the thin piece of folded paper that is part of every drug prescription box. Usually the text is in very small print and typically provides information about dosages, side effects, storage instructions and much more. They are hard to read and understand and requires some effort to get answers to common questions a patient might have. What if we could create a product that answers these questions and actually makes the medical information more accessible and easier to understand - enter PillPal!"
      ],
      "metadata": {
        "id": "oZ-FEcb8SjNy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the process of building PillPal, we will go through the following stages and on the way learn more about LLM Apps and their typical life-cycle.\n",
        "\n",
        "1. BUILD the app: this is where we first test the idea, try multiple options for various parts of the pipeline till we are satisfied to a reasonable extent that the product works.\n",
        "2. DEPLOY & MONITOR the app: this is where we want to make the app available to our first users, monitor it's behaviour and discover edge cases.\n",
        "3. EVALUATE & IMPROVE the app: this is the final and ongoing stage where we will build an evaluation suite that constantly checks whether our app is working as expected and perform experiments."
      ],
      "metadata": {
        "id": "0fEvG823VR9F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use Langchain to develop the app, deploy it as a Telegram bot/ Website and use Langsmith to monitor, evaluate and improve the product. Please consider PillPal as the chosen example/ case study to illustrate the process but feel free to adapt this project ot build any app of your choice!\n",
        "\n",
        "# # üë®‚Äçüéì Learner Project\n",
        "\n",
        "In the project walkthrough session we will go through the various steps of the project with an example of one of the drugs. The learner project is to implement the same steps for a different drug OR any other PDF of your choice. Concretely, here are steps that you need to do:\n",
        "\n",
        "1. Identify a PDF that you would like to ask questions about. Suggestion: choose another drug or medication that you are familiar with.\n",
        "2. Upload and use that PDF as you go through this notebook.\n",
        "3. Make use of the sections and flow as a guide but you are expected to update and write code to complete the RAG project for your PDF.\n",
        "4. Feel free to adapt any sections to add more functionality or adapt for your use case/ PDF."
      ],
      "metadata": {
        "id": "I2S69-R7aK0k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 0: Necessary libraries and setup"
      ],
      "metadata": {
        "id": "ruybS1MUZ3--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the necessary libraries to get it out of the way\n",
        "!pip install langchain\n",
        "!pip install langchain-community\n",
        "!pip install langchain-openai\n",
        "!pip install faiss-cpu\n",
        "!pip install pymupdf\n",
        "!pip install grandalf\n",
        "!pip install gradio"
      ],
      "metadata": {
        "id": "JbQIsr6r6kOq",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### OpenAI Initialization"
      ],
      "metadata": {
        "id": "9kCSk1SucCyS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You will be provided an API key by the Course Manager. If you are not able to find it, please reach out on ask for help on the Discussion forums.\n",
        "\n",
        "Please note that this is a shared API key for the entire class, so please make sure to use it responsibly. We will also show you steps below to add it to your secret keys so that it is not revealed in any way."
      ],
      "metadata": {
        "id": "8SmzMEqgcFg2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once you have obtained the API key, you can add it to the Google Colab instance by adding it to the Secrets section as shown below. Please use the variable name 'OPENAI_API_KEY' and paste the value that you copied before.\n",
        "\n",
        "![Add to Google Colab](https://i.ibb.co/Cb57Sxq/Xnapper-2024-06-09-21-42-30.png)"
      ],
      "metadata": {
        "id": "4R3CUqKij-mo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Building the PillPal bot"
      ],
      "metadata": {
        "id": "XS1dSf8E6ZUS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The patient information leaflets/ drug booklets are typically available in the form of PDFs from the drug manufacturers website or from a central repository like in the [UK](https://www.medicines.org.uk/) or the [US](https://dailymed.nlm.nih.gov/). While you should be able to find them relatively easily through an Internet Search, we also include the medical booklet PDFs for two drugs with this code repository to use in the project."
      ],
      "metadata": {
        "id": "0It6mAFmW4Qj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One of the best aspects of using Langchain is that it provides a lot of in-built integrations for most common development tasks while building LLM apps. We will make use of several of them during this project, starting with the PDF loader which we will use to read in the medical booklet PDF file. For this example, we have chosen the PIL for Ozempic - a new drug that decreses the risk of heart disease in overweight patients but has been in the news recently for also being a weight loss solution."
      ],
      "metadata": {
        "id": "DXAlB_g8ZGM6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we are still in the notebook environment - please follow the below steps to upload the PDF document into the Colab environment.\n",
        "\n",
        "![Upload PDF](https://i.ibb.co/jgDBwY4/Xnapper-2024-06-09-21-58-08.png)"
      ],
      "metadata": {
        "id": "vtqgnSjFZ7RP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìù Learner Task:\n",
        "\n",
        "In the following section, you have to load in your PDF using any of the document loaders available from the Langchain Community package. For example: you could use the `PyMuPDFLoader` for managing PDFs.\n",
        "\n",
        "Your code should do the following:\n",
        "- Load the PDF\n",
        "- Identify the number of pages\n",
        "- Print one of the pages with the associated metadata"
      ],
      "metadata": {
        "id": "S505i8I5MbI2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Your code here"
      ],
      "metadata": {
        "id": "n2nJ6tyN7758"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The additional metadata like page number and title is relevant to our product because when we answer questions: we can also point users to specific sections of the original document which they can refer to for more clarity."
      ],
      "metadata": {
        "id": "H1ksk-czaxN5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our next step is to index this entire document which we do with the help of OpenAI embeddings. However, before we do that we need to split document into chunks so that at the time of retrieval we are identifying the correct parts of the document. We use one of the in-built Langchain components that allows us to split based on characters based on our specifications.\n",
        "\n",
        "We have chosen to split the entire document into chunks of length 2000 characters and also ensure an overlap of 200 characters. This makes sure that we are not loosing any information when splitting up the document. As we will see later, this is one of the parameters that we can control and could have an influence on the performance of our product."
      ],
      "metadata": {
        "id": "8eI1CZqcbF7S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìù Learner Task:\n",
        "\n",
        "In the following section, you have to decide the best chunking strategy for your PDF and use case. For instance, you could make use of the `RecursiveCharacterTextSplitter` to work with text documents. It's important to also decide what are reasonable values for `chunk_size` and `chunk_overlap`. In the project walkthrough we provide some suggested values."
      ],
      "metadata": {
        "id": "PbkgyfD9NryJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Your code here"
      ],
      "metadata": {
        "id": "4xMZXPdM9fae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are optimising for retrieval - i.e. how fine-grained is the context that we can retrieve so that we can answer the question that our user is asking. In this case, it might be better to have a smaller chunk because you are then narrowing down on the perfect part of the text where this information is present."
      ],
      "metadata": {
        "id": "7tqcP7ljBQsK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "from google.colab import userdata\n",
        "\n",
        "## Define the embedding model below by using the `text-embedding-3-small` model\n",
        "\n",
        "embedding_model = OpenAIEmbeddings(model='text-embedding-3-small', openai_api_key=userdata.get('OPENAI_API_KEY'))"
      ],
      "metadata": {
        "id": "UBVkp6TbaZDr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A huge advantage of using Langchain is that it provides integrations with several types of vector databases like Chroma, Pinecone and more. In this project, we make use of the FAISS library from Facebook/Meta as a simple choice. In the following section, we combine the steps of generating the embedding value for each document chunk and then also storing it into the FAISS vector database."
      ],
      "metadata": {
        "id": "IDaUK7NbE8J4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "vector_store = FAISS.from_documents(documents=splits, embedding=embedding_model)"
      ],
      "metadata": {
        "id": "NDV_HnMFFJfb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have our documents indexed and we can directly start retrieving documents based on a similarity search with a target query."
      ],
      "metadata": {
        "id": "_nI7cwlNdiBr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "search_result = vector_store.similarity_search_with_relevance_scores(query=\"What is the recommended dosage?\", k=4)\n",
        "search_result"
      ],
      "metadata": {
        "id": "x1_wRhCQJBLJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this case, we have chosen to use cosine similarity as our distance metric and the similarity values are also shown along with the retrieved results. You will notice that not many of the retrieved chunks actually contain the text \"Weight Loss\" - and this is because we are not doing a word-based search but rather semantic search. This is what vector databases allow us to do. However, it is also possible that at times we want to match exact query terms and then we will follow a hybrid search approach. This is again one of the levers that we have to expriment with to determine what is necessary for our product. So it's important that you are able to determine what is the best method for you to choose - depends a lot on your use-case."
      ],
      "metadata": {
        "id": "SxPewWEYLG1R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating the QnA RAG chain"
      ],
      "metadata": {
        "id": "H_MbjNwSaEOn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's move on to complete our product by integrating with the LLM. Here again, we rely heavily on the building blocks that Langchain already provides us to put together the entire chain.\n",
        "\n",
        "The chain consists of multiple parts - a Retriever, followed by a Prompt Template where the retrieved documents are added and then the call to the LLM. The response from the LLM is what we finally show as output to the user. There are multiple steps in creating a RAG application. We break it down to make it easier to understand and follow."
      ],
      "metadata": {
        "id": "i7eiBrgeetbs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìù Learner Task:\n",
        "Creating the Prompt Template. In the section below, please create the PromptTemplate that will be used for your RAG application. The `ChatPromptTemplate` has become the standardized way to use chat-based LLMs and consists of a `SystemMessage`, followed by the `HumanMessage` with the response from the LLM stored in the `AIMessage`. We have provided the necessary imports in the below cell and request you to create the variable `qna_prompt_template` that will be used in the application."
      ],
      "metadata": {
        "id": "HlqW4TbcOfBw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain.prompts import SystemMessagePromptTemplate\n",
        "from langchain.prompts import HumanMessagePromptTemplate\n",
        "\n",
        "## Your code here"
      ],
      "metadata": {
        "id": "5cv_Fi0IPfn5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After the PromptTemplate, the next step is to define the LLM Model that you would like to run with. We provide the OpenAI API models as the default option and make use of `gpt-4o-mini`. We also set the `temperature` value to 0 as we do not want the model to be creative but rather answer based on the retrieved context from the PDF."
      ],
      "metadata": {
        "id": "iumOVFV4Pk_X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(model_name=\"gpt-4o-mini\",\n",
        "                 temperature=0,\n",
        "                 openai_api_key=userdata.get('OPENAI_API_KEY'))"
      ],
      "metadata": {
        "id": "9-wDIb1gQI7E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next step is to initilize the retriever part of our application that will fetch the relevant documents from the PDF. Recall that we have already created the embeddings for our document chunks and stored it using the FAISS vectorstore. In this step, we only specify that vectorestore to be our retriever."
      ],
      "metadata": {
        "id": "nDlnYaodQPeU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = vector_store.as_retriever()"
      ],
      "metadata": {
        "id": "hz_SleAiQrTi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "One additional step that we need to also consider is that when the documents are retrieved from the FAISS vectorstore, they are in the form of a list of documents. These document objects contain a lot more information other than the content like Metadata. Assuming we are performing a simple retrieval, the context that we want to pass to the LLM is only the text content. Therefore we write an additional function that combines only the retrieved text content that can be used to pass in the context of our PromptTemplate.\n",
        "\n",
        "Please note that you could also choose to filter the retrieved documents based on the Metadata. For instance, if you only want to see results from Page 5 and beyond of the PDF - then you can also adapt this function to reflect that."
      ],
      "metadata": {
        "id": "zEtojMFZQyjR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
      ],
      "metadata": {
        "id": "b64wwSooQv2f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The final stage is to put together the various elements that we have defined above to create our pipeline. The key aspect to note here is that the input question is used twice during the process -> at the first instance to retrieve a list of the relevant context and the second instance when it is passed to the LLM as the question that needs to be answered.\n",
        "\n",
        "So we make use of the `itemgetter` to retrieve the content of the question and pass to the retriever to get the relevant context. On the other hand, we make use of the `RunnablePassthrough` class to directly pass the input question to the subsequent operation. At the end we also add the `StrOuputParser` to get the actual text message of the response rather than the whole AIMessage."
      ],
      "metadata": {
        "id": "0VVUVqJARaYJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from operator import itemgetter\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "rag_chain = (\n",
        "    {\"context\": itemgetter(\"input\") | retriever | format_docs,\n",
        "     \"question\": RunnablePassthrough()}\n",
        "    | qna_prompt_template\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")"
      ],
      "metadata": {
        "id": "VI7UOE70LleJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please check that your entire chain works by providing your questions and invoking the RAG chain."
      ],
      "metadata": {
        "id": "t5YW6DF-Sog3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = ## Your question\n",
        "rag_chain.invoke({\"input\": question})"
      ],
      "metadata": {
        "id": "ftZIkyQISoAt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìù Learner Task:\n",
        "If you run into errors as you develop, visualizing the chain is an easy way to discover any issues. This visualization helps us understand the entire flow of operations within our application. By graphically representing the connections and interactions between different components we can see how the data flows and in which shape from one path tothe next. It allows for easier debugging and we might be able to pinpoint where modifications might be necessary."
      ],
      "metadata": {
        "id": "PBbDadkAT1zB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print (rag_chain.get_graph().print_ascii())"
      ],
      "metadata": {
        "id": "l50o8HECL53q",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Hopefully, you have managed to build the skeleton for your product. At first just make sure that everything is working in an end to end fashion. After you have achieved that you can move in the direction of improving the answers from the application. This is when you can start experimenting with different aspects of your pipeline to go in the direction of a better solution.\n",
        "\n",
        "We would recommend adapting the following options:\n",
        "\n",
        "- Chunk size:\n",
        "- Retrieval strategy/ metric:\n",
        "- Choice of LLM:"
      ],
      "metadata": {
        "id": "VwTao9CgfNrN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "At the end of this stage, you should be able to have a working prototype of your solution that does reasonably well for the use-case you have in mind.\n",
        "\n",
        "We are still relying on a vibe/gut-feel for the product and cannot rely on quantified metrics. But hopefully you can feel reasonably confident to launch an alpha version of your product to a select group of customers."
      ],
      "metadata": {
        "id": "IvhtzIRbfrec"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Deploying the PillPal bot"
      ],
      "metadata": {
        "id": "NlvAHj4bPl6Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before deploying our product, one of the important steps is to enable tracking and monitoring all the user queries and LLM responses. This is made very easy for us with the help of Langsmith - another part of the Langchain ecosystem.\n",
        "\n",
        "This is one of the cornerstones of our strategy to get towards a more valuable and performing LLM app.\n",
        "\n",
        "Please sign-up on the Langsmith [Website](https://smith.langchain.com/) and you should have access. Next, please navigate to the Settings page from the left side navigation menu and then create your API key there and copy it.\n",
        "\n",
        "<a href=\"https://ibb.co/fN8s3Yp\"><img src=\"https://i.ibb.co/ZgH0Q68/Screenshot-2024-07-26-at-16-12-48.png\" alt=\"Screenshot-2024-07-26-at-16-12-48\" border=\"0\"></a>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "After that, you can return to the main page and click the New Project icon and copy the environment details that you see.\n",
        "\n",
        "<a href=\"https://ibb.co/th2CQhd\"><img src=\"https://i.ibb.co/yRksdR3/Screenshot-2024-07-26-at-16-12-59.png\" alt=\"Screenshot-2024-07-26-at-16-12-59\" border=\"0\"></a>"
      ],
      "metadata": {
        "id": "0X3ZhG3_PqrI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['LANGSMITH_TRACING'] = 'true'\n",
        "os.environ['LANGSMITH_ENDPOINT'] = \"https://api.smith.langchain.com\"\n",
        "os.environ['LANGSMITH_API_KEY'] = userdata.get('LANGSMITH_API_KEY')\n",
        "os.environ['LANGSMITH_PROJECT'] = \"YOUR_PROJECT_NAME\""
      ],
      "metadata": {
        "id": "WeuJlFzdOnBA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can deploy our LLM app in two different ways -\n",
        "\n",
        "1. A website based chatbot experience - this will be done with the help of Gradio and you do not need to leave the Colab notebook environment\n",
        "2. A telegram chatbot that will run on Github Codespaces (or locally) and will need you to leave the Colab environment"
      ],
      "metadata": {
        "id": "-AylFlTLPyp5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Deployed as website chatbot\n",
        "\n",
        "We can deploy our QnA bot on a dedicated website using Gradio, an easy-to-use library for creating interactive machine learning interfaces. One significant advantage of using Gradio in our project is that it integrates seamlessly within the Colab notebook environment. This means you don't need to leave Colab to see your bot in action; you can run it directly from the notebook. Gradio also provides a dedicated URL that you can share with potential beta testers of the QnA bot to get feedback."
      ],
      "metadata": {
        "id": "4TQ_kDighGgL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.schema import AIMessage, HumanMessage\n",
        "import openai\n",
        "import gradio as gr\n",
        "\n",
        "def predict(message, history):\n",
        "    return rag_chain.invoke({\"input\":message})\n",
        "\n",
        "gr.ChatInterface(predict).launch()"
      ],
      "metadata": {
        "id": "DDy0V7_eoEYt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### [OPTIONAL] Deployed as Telegram bot\n",
        "\n",
        "For a wider launch of your LLM app, you can easily make it available as a bot on Telegram - a messenger platform that is used by 500 million users every day. This is useful in the beta/alpha phase because it's way easier to get users to try it in an app that they already use every day. This need not be just Telegram but other popular messengers like Whatsapp, Signal etc. We chose Telegram as it's free and easy to setup.\n",
        "\n",
        "In order to deploy using this option, we will make use of Github codespaces as it's difficult to run it within a synchronous coding environment like Colab. The instructions and code for this will be provided in the live project session."
      ],
      "metadata": {
        "id": "_6TnwMSgpNps"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please go to the Github repository: https://github.com/sidhusmart/uplimit_langchain.git and clone it so that you can work on it directly. Please note that code in this repository may change frequently so we request you to work only on your cloned version.\n",
        "\n",
        "Once you have cloned it, then you can launch your own Codespaces instance by following the steps as shown in the screenshot. This creates a virtual Visual Studio Code environment in the cloud where we can test our Telegram bot.\n",
        "\n",
        "<a href=\"https://ibb.co/p02fJ1t\"><img src=\"https://i.ibb.co/BzLyqfX/Screenshot-2024-07-26-at-16-05-46.png\" alt=\"Screenshot-2024-07-26-at-16-05-46\" border=\"0\"></a>"
      ],
      "metadata": {
        "id": "mmX9Z7lZ8eWT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The two important files for you to take a look are `langchain_rag_app.py` and `telegram_response_bot.py` The first python file contains the same RAG functionality that we already built but packaged in functions. The telegram response bot contains the necessary code required to run the Telegram bot.\n",
        "\n",
        "In order to setup and run this app, we have to add the necessary API keys. This is the same process as what we have done previously in the Colab notebook but since we are executing this code in a new environment, we have to add the necessary secrets there as well.\n",
        "\n",
        "We do this via the `.env.dev` file. This file does not exist in the repository by default but instead we have provided the template file called `.env.template`. You need to create a copy of this file and rename it to be `.env.dev` that will contain all the API keys you are going to use during development of the app. Please note that this file is not checked in as we do not want to commit the secrets!  \n",
        "\n",
        "You can observe that there are several environment variables defined in the .env file - most of which we have already setup. You can use the same values for those keys again.\n",
        "\n",
        "However, there is one additional API key that is required to work with the Telegram API. In order to get the Telegram bot API key, you need to follow these steps:\n",
        "\n",
        "1. You must have a Telegram account and access to the App - Web or Mobile versions would also work.\n",
        "2. On the Telegram app, you need to search and look for a contact called the Botfather.\n",
        "3. You can use the `/start` command to get a list of options or directly use the `/newbot` command to create a new bot for yourself.\n",
        "4. Then answer the set of questions by providing a name for and username for your bot.\n",
        "5. At the end you will be provided with an API token that you can use to work with the bot. This API token is what we need to replace in the .env.dev file. `telegram_response_bot.py` file.\n",
        "\n",
        "<a href=\"https://imgbb.com/\"><img src=\"https://i.ibb.co/f2MqxCd/Screenshot-2024-07-26-at-14-23-02.png\" alt=\"Screenshot-2024-07-26-at-14-23-02\" border=\"0\"></a>"
      ],
      "metadata": {
        "id": "zC344Wj19pcE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once you have performed both these steps, you are now ready to run your RAG app as a Telegam bot. You need to run the command `python telegram_response_bot.py ` from the command line of your Github Space and this starts a thread that listens for messages on your bot. Once a message is received, it then hands off to the RAG app and provides the generated response.\n",
        "\n",
        "You can easily share this Pillpal bot with your friends and family and ask them to try it!"
      ],
      "metadata": {
        "id": "aOutTlUR_zmC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Evaluation of PillPal"
      ],
      "metadata": {
        "id": "Y_4zSQWPQM-R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have deployed and made our app live, we need to consistently monitor how it performs. This is also a way for us to understand what questions are being asked by our users and whether the bot is responding correctly or not. For this we will make use of the Langsmith part of the Langchain library."
      ],
      "metadata": {
        "id": "m9wzLbcQptfZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As the first step, we have to create a validation dataset that we can use for evaluation. There are several ways to go about doing this:\n",
        "\n",
        "1. Launch your app in beta and get some trial users to start interacting with the bot/chat interface and identify common patterns and questions that are being asked. Manually, write your own answers to these questions and perform evaluations using that.\n",
        "2. Come up with a list of 5-10 questions and manually search for the answers of these questions in the PDF and create this as your evaluation dataset -> this is what we are going to do below.  \n",
        "3. You could also make use of another LLM to come up with these questions and answers which is called creating a synthetic dataset. This would work well if you use higher level models like GPT-4 but you need to keep in mind that they have to be relevant to the context of your leaflet."
      ],
      "metadata": {
        "id": "Gvj_e6UYon-V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating a base evaluation dataset"
      ],
      "metadata": {
        "id": "b-H03qVVaMkH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To ensure that our application is functioning correctly and effectively answering questions, we have implemented a basic sanity check. This involves a set of 8-10 questions that I have manually curated and answered after thoroughly reviewing the content of the PDF document. These questions are designed to cover a wide range of topics and complexities within the document, ensuring that they test various aspects of our QnA bot's capabilities. By comparing the answers generated by our bot to these manually prepared responses, we can gauge the accuracy and reliability of our system. This step is crucial as it helps us identify any discrepancies or areas needing improvement before the application is deployed for wider use. Additionally, it provides an initial layer of validation and builds confidence in the application's performance among users and stakeholders."
      ],
      "metadata": {
        "id": "6xTfANXXXd5P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìù Learner Task:\n",
        "\n",
        "Please create a set of question and correct answer pairs here to use as your evaluation dataset. We have provided the first two questions as examples but feel free to overwrite and add your own."
      ],
      "metadata": {
        "id": "Vv6vi_EAT89n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "examples = [\n",
        "    (\n",
        "        \"Can it be taken by pregnant women?\",\n",
        "        \"No, it should not be taken by pregnant women before seeking the doctors advice.\",\n",
        "    ),\n",
        "    (\n",
        "        \"Can it be given to children?\",\n",
        "        \"No, it should be taken by children aged less than 6 years old. Children above the age of 6 must follow the prescribed dosage - half a tablet twice daily for children between 6 and 12 years.\",\n",
        "    )\n",
        "]"
      ],
      "metadata": {
        "id": "wXFxOClWQPbe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once we have successfully created our dataset, the next step involves registering it with the Langsmith client. This process integrates the dataset into our evaluation framework. By registering the dataset, we effectively add it to the list of datasets available for testing and validating our model. In general, you might create multiple datasets to test various aspects of the QnA bot and might run different types of evaluations on it.\n",
        "\n",
        "In this case, we are running a test for general correctness of the QnA bot - while this does include checks for hallucinations -> we have added examples where the bot should not have an answer and must say so. But there are additional datasets that you might create with separate evaluation metrics to explicitly check for hallucination and fact-checking."
      ],
      "metadata": {
        "id": "x82VVhfGqJ9S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langsmith import Client\n",
        "\n",
        "client = Client()"
      ],
      "metadata": {
        "id": "hGqC94J7qSEn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langsmith import Client\n",
        "\n",
        "client = Client()\n",
        "\n",
        "dataset_name = \"PillPall Accuracy Test Dataset\"\n",
        "dataset = client.create_dataset(dataset_name=dataset_name)\n",
        "inputs, outputs = zip(\n",
        "    *[({\"input\": input}, {\"expected\": expected}) for input, expected in examples]\n",
        ")\n",
        "client.create_examples(inputs=inputs, outputs=outputs, dataset_id=dataset.id)"
      ],
      "metadata": {
        "id": "2EbuuZNMqTRk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once the above cells are run, you should now be able to see the dataset created in the dataset folder in your Langsmith project.\n",
        "\n",
        "![QA Dataset](https://i.ibb.co/sH5dv86/Xnapper-2024-06-11-20-58-53.png)"
      ],
      "metadata": {
        "id": "EqUUjzJUq_F5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's run the evaluation of our app using this dataset.\n",
        "\n",
        "By executing the code below, we will first be calling our app for each of the questions in our QA dataset. Once the answers have been generated, we need to evaluate it with the golden standard answers that we have written manually. But the big question is always: how can we compare two texts, what metrics shall we use?\n",
        "\n",
        "One of the easier ways to do this is by using another LLM to compare the two responses and tell you whether they match or not. This is also referred to as 'LLM-as-a-judge'. In this case, since we have only 5-10 examples you can easily do this manually but typically your evaluation datasets will contain a lot of examples and this method would not work.\n",
        "\n",
        "We will use the off-the-shelf evaluator that Langsmith provides us called `cot_qa` that stands for Chain of Thought Question Answer. This basically refers to a pre-filled prompt that is used when asking the \"Judge LLM\" to rate whether the two answers are comparable or not.\n",
        "\n",
        "Typically we make use of a bigger, more powerful LLM to act as the judge for what the smaller, cheaper LLM has generated. However, since our dataset is small and quite easy we stick with `gpt-4o-mini`"
      ],
      "metadata": {
        "id": "kcZrdxNVrci2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langsmith import Client\n",
        "from langsmith.evaluation import LangChainStringEvaluator, evaluate\n",
        "\n",
        "eval_llm = ChatOpenAI(temperature=0.0, model=\"gpt-4o-mini\", openai_api_key=userdata.get('OPENAI_API_KEY'))\n",
        "cot_qa_evaluator = LangChainStringEvaluator(\"cot_qa\", config={\"llm\": eval_llm})\n",
        "\n",
        "client = Client()\n",
        "evaluate(\n",
        "    rag_chain,\n",
        "    data=dataset_name,\n",
        "    evaluators=[cot_qa_evaluator],\n",
        ")"
      ],
      "metadata": {
        "id": "vMe1SzrvrKMs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's run the evaluation on the dataset. You get the links to where you can track the run and at the end the output is also printed here."
      ],
      "metadata": {
        "id": "nd_6YQfQsvgw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After initiating the evaluation of our QnA bot using the dataset we registered, you will be able to observe several key performance metrics. These metrics include the percentage of questions answered correctly, which provides a direct measure of the bot's accuracy. You will also see the duration of the test, which gives insight into the efficiency of the bot under testing conditions. Additionally, the evaluation will report on costs and latency, offering a broader view of the bot's operational performance.\n",
        "\n",
        "This initial evaluation serves as a preliminary indication of how well your bot is performing. As the bot encounters more users and a diverse array of questions, it's crucial to continually update and adapt your baseline test dataset. This iterative process ensures that the bot remains effective and responsive to the evolving needs and contexts it will encounter in real-world applications.\n",
        "\n",
        "![Evaluation Test results](https://i.ibb.co/txtzhtD/Xnapper-2024-06-16-11-32-28.png)"
      ],
      "metadata": {
        "id": "C5PPIGVIYU8Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìù Learner Task:\n",
        "\n",
        "Now that you have an end to end RAG chatbot up and running, you can try to stress-test your bot by asking all kinds of questions - both relevant and irrelevant. What kind of responses does it generate? How often does it hallucinate?"
      ],
      "metadata": {
        "id": "Bj_PnIs8aouf"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CgQ9dSJJa61b"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}