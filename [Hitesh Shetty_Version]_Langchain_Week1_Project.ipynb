{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HiteshShettyK90/langchain-apps/blob/main/%5BHitesh%20Shetty_Version%5D_Langchain_Week1_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook is part of the course: [LLM Apps with Langchain](https://uplimit.com/course/llm-apps-with-langchain) and is created by Sidharth Ramachandran as the project for Week 1 of the course."
      ],
      "metadata": {
        "id": "h-T0u92GVJE6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Let's build \"PillPal\" - a Telegram bot and website that allows users to ask questions based on the Patient Information Leaflet (PIL) or drug guide that comes with any medication."
      ],
      "metadata": {
        "id": "BmgRqB0z3JLd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Package Information Leaflet](https://i.ibb.co/ZSdGyjV/ima-image-36904.jpg)"
      ],
      "metadata": {
        "id": "zcmYA3M8aGp3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I'm sure you have all come across the thin piece of folded paper that is part of every drug prescription box. Usually the text is in very small print and typically provides information about dosages, side effects, storage instructions and much more. They are hard to read and understand and requires some effort to get answers to common questions a patient might have. What if we could create a product that answers these questions and actually makes the medical information more accessible and easier to understand - enter PillPal!"
      ],
      "metadata": {
        "id": "oZ-FEcb8SjNy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the process of building PillPal, we will go through the following stages and on the way learn more about LLM Apps and their typical life-cycle.\n",
        "\n",
        "1. BUILD the app: this is where we first test the idea, try multiple options for various parts of the pipeline till we are satisfied to a reasonable extent that the product works.\n",
        "2. DEPLOY & MONITOR the app: this is where we want to make the app available to our first users, monitor it's behaviour and discover edge cases.\n",
        "3. EVALUATE & IMPROVE the app: this is the final and ongoing stage where we will build an evaluation suite that constantly checks whether our app is working as expected and perform experiments."
      ],
      "metadata": {
        "id": "0fEvG823VR9F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use Langchain to develop the app, deploy it as a Telegram bot/ Website and use Langsmith to monitor, evaluate and improve the product. Please consider PillPal as the chosen example/ case study to illustrate the process but feel free to adapt this project ot build any app of your choice!\n",
        "\n",
        "# # üë®‚Äçüéì Learner Project\n",
        "\n",
        "In the project walkthrough session we will go through the various steps of the project with an example of one of the drugs. The learner project is to implement the same steps for a different drug OR any other PDF of your choice. Concretely, here are steps that you need to do:\n",
        "\n",
        "1. Identify a PDF that you would like to ask questions about. Suggestion: choose another drug or medication that you are familiar with.\n",
        "2. Upload and use that PDF as you go through this notebook.\n",
        "3. Make use of the sections and flow as a guide but you are expected to update and write code to complete the RAG project for your PDF.\n",
        "4. Feel free to adapt any sections to add more functionality or adapt for your use case/ PDF."
      ],
      "metadata": {
        "id": "I2S69-R7aK0k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 0: Necessary libraries and setup"
      ],
      "metadata": {
        "id": "ruybS1MUZ3--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the necessary libraries to get it out of the way\n",
        "!pip install langchain\n",
        "!pip install langchain-community\n",
        "!pip install langchain-openai\n",
        "!pip install faiss-cpu\n",
        "!pip install pymupdf\n",
        "!pip install grandalf\n",
        "!pip install gradio"
      ],
      "metadata": {
        "id": "JbQIsr6r6kOq",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8cc5e1f-002f-4c27-b7a9-6eccb2eecc9a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.23)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.51 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.52)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.31)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.3)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.40)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain) (4.13.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.16)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.1.31)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.8)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.51->langchain) (3.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n",
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.21-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.51 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.52)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.23 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.23)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.40)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (9.1.2)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
            "  Downloading pydantic_settings-2.9.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.31)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: numpy<3,>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.19.0)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.23->langchain-community) (0.3.8)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.23->langchain-community) (2.11.3)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain-community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain-community) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain-community) (4.13.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (3.10.16)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.23.0)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2025.1.31)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.2.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.0.8)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.51->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.23->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.23->langchain-community) (2.33.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.3.1)\n",
            "Downloading langchain_community-0.3.21-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading pydantic_settings-2.9.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: python-dotenv, mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, dataclasses-json, langchain-community\n",
            "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.0 langchain-community-0.3.21 marshmallow-3.26.1 mypy-extensions-1.0.0 pydantic-settings-2.9.1 python-dotenv-1.1.0 typing-inspect-0.9.0\n",
            "Collecting langchain-openai\n",
            "  Downloading langchain_openai-0.3.14-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting langchain-core<1.0.0,>=0.3.53 (from langchain-openai)\n",
            "  Downloading langchain_core-0.3.54-py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.68.2 in /usr/local/lib/python3.11/dist-packages (from langchain-openai) (1.75.0)\n",
            "Collecting tiktoken<1,>=0.7 (from langchain-openai)\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.53->langchain-openai) (0.3.31)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.53->langchain-openai) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.53->langchain-openai) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.53->langchain-openai) (6.0.2)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.53->langchain-openai) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.53->langchain-openai) (4.13.2)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.53->langchain-openai) (2.11.3)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (0.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (4.67.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2.32.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.68.2->langchain-openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.68.2->langchain-openai) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.68.2->langchain-openai) (1.0.8)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.68.2->langchain-openai) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.53->langchain-openai) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.53->langchain-openai) (3.10.16)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.53->langchain-openai) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.53->langchain-openai) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<1.0.0,>=0.3.53->langchain-openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<1.0.0,>=0.3.53->langchain-openai) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<1.0.0,>=0.3.53->langchain-openai) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (2.3.0)\n",
            "Downloading langchain_openai-0.3.14-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.3.54-py3-none-any.whl (433 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m433.9/433.9 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m42.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tiktoken, langchain-core, langchain-openai\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.52\n",
            "    Uninstalling langchain-core-0.3.52:\n",
            "      Successfully uninstalled langchain-core-0.3.52\n",
            "Successfully installed langchain-core-0.3.54 langchain-openai-0.3.14 tiktoken-0.9.0\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.10.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
            "Downloading faiss_cpu-1.10.0-cp311-cp311-manylinux_2_28_x86_64.whl (30.7 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m30.7/30.7 MB\u001b[0m \u001b[31m65.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.10.0\n",
            "Collecting pymupdf\n",
            "  Downloading pymupdf-1.25.5-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.4 kB)\n",
            "Downloading pymupdf-1.25.5-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (20.0 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m90.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pymupdf\n",
            "Successfully installed pymupdf-1.25.5\n",
            "Collecting grandalf\n",
            "  Downloading grandalf-0.8-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from grandalf) (3.2.3)\n",
            "Downloading grandalf-0.8-py3-none-any.whl (41 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m41.8/41.8 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: grandalf\n",
            "Successfully installed grandalf-0.8\n",
            "Collecting gradio\n",
            "  Downloading gradio-5.25.2-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting aiofiles<25.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
            "  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting gradio-client==1.8.0 (from gradio)\n",
            "  Downloading gradio_client-1.8.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting groovy~=0.1 (from gradio)\n",
            "  Downloading groovy-0.1.2-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.30.2)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.16)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.1.0)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.3)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.18 (from gradio)\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Collecting ruff>=0.9.3 (from gradio)\n",
            "  Downloading ruff-0.11.6-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n",
            "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.46.2-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.2)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.13.2)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.34.2-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.8.0->gradio) (2025.3.2)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.8.0->gradio) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.8)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading gradio-5.25.2-py3-none-any.whl (46.9 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m46.9/46.9 MB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.8.0-py3-none-any.whl (322 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m322.2/322.2 kB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
            "Downloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading groovy-0.1.2-py3-none-any.whl (14 kB)\n",
            "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading ruff-0.11.6-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.5 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m109.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
            "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading starlette-0.46.2-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
            "Downloading uvicorn-0.34.2-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub, uvicorn, tomlkit, semantic-version, ruff, python-multipart, groovy, ffmpy, aiofiles, starlette, safehttpx, gradio-client, fastapi, gradio\n",
            "Successfully installed aiofiles-24.1.0 fastapi-0.115.12 ffmpy-0.5.0 gradio-5.25.2 gradio-client-1.8.0 groovy-0.1.2 pydub-0.25.1 python-multipart-0.0.20 ruff-0.11.6 safehttpx-0.1.6 semantic-version-2.10.0 starlette-0.46.2 tomlkit-0.13.2 uvicorn-0.34.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### OpenAI Initialization"
      ],
      "metadata": {
        "id": "9kCSk1SucCyS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You will be provided an API key by the Course Manager. If you are not able to find it, please reach out on ask for help on the Discussion forums.\n",
        "\n",
        "Please note that this is a shared API key for the entire class, so please make sure to use it responsibly. We will also show you steps below to add it to your secret keys so that it is not revealed in any way."
      ],
      "metadata": {
        "id": "8SmzMEqgcFg2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once you have obtained the API key, you can add it to the Google Colab instance by adding it to the Secrets section as shown below. Please use the variable name 'OPENAI_API_KEY' and paste the value that you copied before.\n",
        "\n",
        "![Add to Google Colab](https://i.ibb.co/Cb57Sxq/Xnapper-2024-06-09-21-42-30.png)"
      ],
      "metadata": {
        "id": "4R3CUqKij-mo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Building the PillPal bot"
      ],
      "metadata": {
        "id": "XS1dSf8E6ZUS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The patient information leaflets/ drug booklets are typically available in the form of PDFs from the drug manufacturers website or from a central repository like in the [UK](https://www.medicines.org.uk/) or the [US](https://dailymed.nlm.nih.gov/). While you should be able to find them relatively easily through an Internet Search, we also include the medical booklet PDFs for two drugs with this code repository to use in the project."
      ],
      "metadata": {
        "id": "0It6mAFmW4Qj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One of the best aspects of using Langchain is that it provides a lot of in-built integrations for most common development tasks while building LLM apps. We will make use of several of them during this project, starting with the PDF loader which we will use to read in the medical booklet PDF file. For this example, we have chosen the PIL for Ozempic - a new drug that decreses the risk of heart disease in overweight patients but has been in the news recently for also being a weight loss solution."
      ],
      "metadata": {
        "id": "DXAlB_g8ZGM6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we are still in the notebook environment - please follow the below steps to upload the PDF document into the Colab environment.\n",
        "\n",
        "![Upload PDF](https://i.ibb.co/jgDBwY4/Xnapper-2024-06-09-21-58-08.png)"
      ],
      "metadata": {
        "id": "vtqgnSjFZ7RP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìù Learner Task:\n",
        "\n",
        "In the following section, you have to load in your PDF using any of the document loaders available from the Langchain Community package. For example: you could use the `PyMuPDFLoader` for managing PDFs.\n",
        "\n",
        "Your code should do the following:\n",
        "- Load the PDF\n",
        "- Identify the number of pages\n",
        "- Print one of the pages with the associated metadata"
      ],
      "metadata": {
        "id": "S505i8I5MbI2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Your code here\n",
        "from langchain.document_loaders import PyMuPDFLoader\n",
        "import fitz  # PyMuPDF\n",
        "\n",
        "# Load the PDF\n",
        "file_path = \"./sample_data/ozempic.pdf\"\n",
        "loader = PyMuPDFLoader(file_path)\n",
        "documents = loader.load()\n",
        "\n",
        "# Identify number of pages\n",
        "doc = fitz.open(file_path)\n",
        "num_pages = doc.page_count\n",
        "print(f\"Total number of pages: {num_pages}\")\n",
        "\n",
        "# Print content and metadata of a specific page (e.g., page 2)\n",
        "page_number = 1  # 0-indexed, so 1 refers to page 2\n",
        "page = documents[page_number]\n",
        "print(f\"\\n--- ### Content of Page {page_number + 1} ---\\n{page.page_content}\")\n",
        "print(f\"\\n--- >>> Metadata of Page {page_number + 1} ---\\n{page.metadata}\")\n"
      ],
      "metadata": {
        "id": "n2nJ6tyN7758",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12f50f01-9faf-4cf5-e90e-557cd226c283"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of pages: 88\n",
            "\n",
            "--- ### Content of Page 2 ---\n",
            "Injection: 4 mg/3 mL (1.34 mg/mL) available in:\n",
            "‚Ä¢\n",
            "Injection: 8 mg/3 mL (2.68 mg/mL) available in:\n",
            "‚Ä¢\n",
            "CONTRAINDICATIONS\n",
            "‚Ä¢\n",
            "‚Ä¢\n",
            "WARNINGS AND PRECAUTIONS\n",
            "‚Ä¢\n",
            "‚Ä¢\n",
            "‚Ä¢\n",
            "‚Ä¢\n",
            "‚Ä¢\n",
            "‚Ä¢\n",
            "‚Ä¢\n",
            "‚Ä¢\n",
            "‚Ä¢\n",
            "ADVERSE REACTIONS\n",
            "The most common adverse reactions, reported in ‚â•5% of patients treated with OZEMPIC are: nausea,\n",
            "vomiting, diarrhea, abdominal pain and constipation (6.1).\n",
            "To report SUSPECTED ADVERSE REACTIONS, contact Novo Nordisk Inc., at 1-888-693-6742\n",
            "or FDA at 1-800-FDA-1088 or http://www.fda.gov/medwatch\n",
            "DRUG INTERACTIONS\n",
            "Oral Medications: OZEMPIC delays gastric emptying. May impact absorption of concomitantly administered\n",
            "oral medications. Use with caution (7.2).\n",
            "USE IN SPECIFIC POPULATIONS\n",
            "Females and Males of Reproductive Potential: Discontinue OZEMPIC in women at least 2 months before a\n",
            "planned pregnancy due to the long washout period for semaglutide (8.3).\n",
            "See 17 for PATIENT COUNSELING INFORMATION and Medication Guide.\n",
            "Revised: 1/2025\n",
            "FULL PRESCRIBING INFORMATION: CONTENTS*\n",
            "WARNING: RISK OF THYROID C-CELL TUMORS\n",
            "1 INDICATIONS AND USAGE\n",
            "2 DOSAGE AND ADMINISTRATION\n",
            "2.1 Important Administration Instructions\n",
            "2.2 Recommended Dosage\n",
            "3 DOSAGE FORMS AND STRENGTHS\n",
            "Single-patient-use pen that delivers 1 mg per injection (3)\n",
            "Single-patient-use pen that delivers 2 mg per injection (3)\n",
            "Personal or family history of MTC or in patients with MEN 2 (4).\n",
            "Serious hypersensitivity reaction to semaglutide or any of the excipients in OZEMPIC (4).\n",
            "Acute Pancreatitis: Has been observed in patients treated with GLP-1 receptor agonists, including\n",
            "semaglutide. Discontinue if pancreatitis is suspected. (5.2).\n",
            "Diabetic Retinopathy Complications: Has been reported in a clinical trial. Patients with a history of\n",
            "diabetic retinopathy should be monitored (5.3).\n",
            "Never share an OZEMPIC pen between patients, even if the needle is changed (5.4).\n",
            "Hypoglycemia: Concomitant use with an insulin secretagogue or insulin may increase the risk of\n",
            "hypoglycemia, including severe hypoglycemia. Reducing dose of insulin secretagogue or insulin may\n",
            "be necessary (5.5).\n",
            "Acute Kidney Injury Due to Volume Depletion: Monitor renal function in patients reporting adverse\n",
            "reactions that could lead to volume depletion (5.6).\n",
            "Severe Gastrointestinal Adverse Reactions: Use has been associated with gastrointestinal adverse\n",
            "reactions, sometimes severe. OZEMPIC is not recommended in patients with severe gastroparesis\n",
            "(5.7).\n",
            "Hypersensitivity Reactions: Serious hypersensitivity reactions (e.g., anaphylaxis and angioedema)\n",
            "have been reported. Discontinue OZEMPIC if suspected and promptly seek medical advice (5.8).\n",
            "Acute Gallbladder Disease: If cholelithiasis or cholecystitis are suspected, gallbladder studies are\n",
            "indicated (5.9).\n",
            "Pulmonary Aspiration During General Anesthesia or Deep Sedation: Has been reported in patients\n",
            "receiving GLP-1 receptor agonists undergoing elective surgeries or procedures. Instruct patients to\n",
            "inform healthcare providers of any planned surgeries or procedures. (5.10).\n",
            "\n",
            "--- >>> Metadata of Page 2 ---\n",
            "{'producer': 'wkhtmltopdf', 'creator': '', 'creationdate': '2025-02-10T19:21:25-05:00', 'source': './sample_data/ozempic.pdf', 'file_path': './sample_data/ozempic.pdf', 'total_pages': 88, 'format': 'PDF 1.4', 'title': 'These highlights do not include all the information needed to use OZEMPIC¬Æ safely and effectively. See full prescribing information for OZEMPIC.OZEMPIC (semaglutide) injection, for subcutaneous useInitial U.S. Approval: 2017', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': \"D:20250210192125-05'00'\", 'page': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The additional metadata like page number and title is relevant to our product because when we answer questions: we can also point users to specific sections of the original document which they can refer to for more clarity."
      ],
      "metadata": {
        "id": "H1ksk-czaxN5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our next step is to index this entire document which we do with the help of OpenAI embeddings. However, before we do that we need to split document into chunks so that at the time of retrieval we are identifying the correct parts of the document. We use one of the in-built Langchain components that allows us to split based on characters based on our specifications.\n",
        "\n",
        "We have chosen to split the entire document into chunks of length 2000 characters and also ensure an overlap of 200 characters. This makes sure that we are not loosing any information when splitting up the document. As we will see later, this is one of the parameters that we can control and could have an influence on the performance of our product."
      ],
      "metadata": {
        "id": "8eI1CZqcbF7S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìù Learner Task:\n",
        "\n",
        "In the following section, you have to decide the best chunking strategy for your PDF and use case. For instance, you could make use of the `RecursiveCharacterTextSplitter` to work with text documents. It's important to also decide what are reasonable values for `chunk_size` and `chunk_overlap`. In the project walkthrough we provide some suggested values."
      ],
      "metadata": {
        "id": "PbkgyfD9NryJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.schema import Document\n",
        "\n",
        "# Step 1: Extract text from each page into a LangChain Document object\n",
        "documents = []\n",
        "for i in range(doc.page_count):\n",
        "    page_text = doc[i].get_text()\n",
        "    metadata = {\"page_number\": i + 1}\n",
        "    documents.append(Document(page_content=page_text, metadata=metadata))\n",
        "\n",
        "# Step 2: Initialize RecursiveCharacterTextSplitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=2000,\n",
        "    chunk_overlap=200\n",
        ")\n",
        "\n",
        "# Step 3: Split documents into chunks\n",
        "split_docs = text_splitter.split_documents(documents)\n",
        "\n",
        "# Step 4: print first few chunks\n",
        "for i, chunk in enumerate(split_docs[:3]):\n",
        "    print(f\"\\n--- Chunk {i + 1} ---\")\n",
        "    print(chunk.page_content)\n",
        "    print(\"Metadata:\", chunk.metadata)"
      ],
      "metadata": {
        "id": "4xMZXPdM9fae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a745080-2d5d-4fec-ca10-a4170f9ce1e5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Chunk 1 ---\n",
            "OZEMPIC- semaglutide injection, solution  \n",
            "Novo Nordisk\n",
            "----------\n",
            "HIGHLIGHTS OF PRESCRIBING INFORMATION\n",
            "These highlights do not include all the information needed to use OZEMPIC\n",
            " safely and\n",
            "effectively. See full prescribing information for OZEMPIC. \n",
            "OZEMPIC (semaglutide) injection, for subcutaneous use \n",
            "Initial U.S. Approval: 2017\n",
            "WARNING: RISK OF THYROID C-CELL TUMORS\n",
            "See full prescribing information for complete boxed warning.\n",
            "‚Ä¢\n",
            "‚Ä¢\n",
            "RECENT MAJOR CHANGES\n",
            " \n",
            "INDICATIONS AND USAGE\n",
            "OZEMPIC is a glucagon-like peptide 1 (GLP-1) receptor agonist indicated:\n",
            "‚Ä¢\n",
            "‚Ä¢\n",
            "‚Ä¢\n",
            "DOSAGE AND ADMINISTRATION\n",
            "‚Ä¢\n",
            "‚Ä¢\n",
            "‚Ä¢\n",
            "‚Ä¢\n",
            "‚Ä¢\n",
            "‚Ä¢\n",
            "‚Ä¢\n",
            "DOSAGE FORMS AND STRENGTHS\n",
            "Injection: 2 mg/3 mL (0.68 mg/mL) available in:\n",
            "‚Ä¢\n",
            "¬Æ\n",
            "In rodents, semaglutide causes thyroid C-cell tumors. It is unknown whether\n",
            "OZEMPIC causes thyroid C-cell tumors, including medullary thyroid carcinoma\n",
            "(MTC), in humans as the human relevance of semaglutide-induced rodent thyroid\n",
            "C-cell tumors has not been determined (5.1, 13.1).\n",
            "OZEMPIC is contraindicated in patients with a personal or family history of MTC or\n",
            "in patients with Multiple Endocrine Neoplasia syndrome type 2 (MEN 2). Counsel\n",
            "patients regarding the potential risk of MTC and symptoms of thyroid tumors (4,\n",
            "5.1).\n",
            "Warnings and Precautions, Pulmonary During General Anesthesia or Deep Sedation (5.9)................\n",
            "11/2024\n",
            "Indication and Usage (1)‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶ 1/2025\n",
            "Dosage and Administration (2.2) ‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶ 1/2025\n",
            "Warnings and Precautions, Severe Gastrointestinal Adverse Reactions ( 5.7 ) 1/2025\n",
            "Warnings and Precautions, Pulmonary Aspiration During General Anesthesia or Deep Sedation ( 5.10 )\n",
            "1/2025\n",
            "as an adjunct to diet and exercise to improve glycemic control in adults with type 2 diabetes mellitus\n",
            "(1).\n",
            "to reduce the risk of major adverse cardiovascular events in adults with type 2 diabetes mellitus and\n",
            "established cardiovascular disease (1).\n",
            "to reduce the risk of sustained eGFR decline, end-stage kidney disease and cardiovascular death in\n",
            "Metadata: {'page_number': 1}\n",
            "\n",
            "--- Chunk 2 ---\n",
            "established cardiovascular disease (1).\n",
            "to reduce the risk of sustained eGFR decline, end-stage kidney disease and cardiovascular death in\n",
            "adults with type 2 diabetes mellitus and chronic kidney disease (1).\n",
            "Administer once weekly at any time of day, with or without meals (2.1).\n",
            "Start at 0.25 mg once weekly. After 4 weeks, increase the dosage to 0.5 mg once weekly (2.2).\n",
            "If additional glycemic control is needed, increase the dosage to 1 mg once weekly after at least 4\n",
            "weeks on the 0.5 mg dose (2.2).\n",
            "If additional glycemic control is needed, increase the dosage to 2 mg once weekly after at least 4\n",
            "weeks on the 1 mg dosage (2.2).\n",
            "To reduce the risk of sustained eGFR decline, end-stage kidney disease and cardiovascular death,\n",
            "increase the dosage to 1 mg once weekly after at least 4 weeks on the 0.5 mg dosage (1, 2.2).\n",
            "If a dose is missed, administer within 5 days of missed dose (2.1).\n",
            "Inject subcutaneously in the abdomen, thigh, or upper arm (2.1).\n",
            "Single-patient-use pen that delivers 0.25 mg or 0.5 mg per injection (3)\n",
            "Metadata: {'page_number': 1}\n",
            "\n",
            "--- Chunk 3 ---\n",
            "Injection: 4 mg/3 mL (1.34 mg/mL) available in:\n",
            "‚Ä¢\n",
            "Injection: 8 mg/3 mL (2.68 mg/mL) available in:\n",
            "‚Ä¢\n",
            "CONTRAINDICATIONS\n",
            "‚Ä¢\n",
            "‚Ä¢\n",
            "WARNINGS AND PRECAUTIONS\n",
            "‚Ä¢\n",
            "‚Ä¢\n",
            "‚Ä¢\n",
            "‚Ä¢\n",
            "‚Ä¢\n",
            "‚Ä¢\n",
            "‚Ä¢\n",
            "‚Ä¢\n",
            "‚Ä¢\n",
            "ADVERSE REACTIONS\n",
            "The most common adverse reactions, reported in ‚â•5% of patients treated with OZEMPIC are: nausea,\n",
            "vomiting, diarrhea, abdominal pain and constipation (6.1).\n",
            "To report SUSPECTED ADVERSE REACTIONS, contact Novo Nordisk Inc., at 1-888-693-6742\n",
            "or FDA at 1-800-FDA-1088 or http://www.fda.gov/medwatch\n",
            "DRUG INTERACTIONS\n",
            "Oral Medications: OZEMPIC delays gastric emptying. May impact absorption of concomitantly administered\n",
            "oral medications. Use with caution (7.2).\n",
            "USE IN SPECIFIC POPULATIONS\n",
            "Females and Males of Reproductive Potential: Discontinue OZEMPIC in women at least 2 months before a\n",
            "planned pregnancy due to the long washout period for semaglutide (8.3).\n",
            "See 17 for PATIENT COUNSELING INFORMATION and Medication Guide.\n",
            "Revised: 1/2025\n",
            "FULL PRESCRIBING INFORMATION: CONTENTS*\n",
            "WARNING: RISK OF THYROID C-CELL TUMORS\n",
            "1 INDICATIONS AND USAGE\n",
            "2 DOSAGE AND ADMINISTRATION\n",
            "2.1 Important Administration Instructions\n",
            "2.2 Recommended Dosage\n",
            "3 DOSAGE FORMS AND STRENGTHS\n",
            "Single-patient-use pen that delivers 1 mg per injection (3)\n",
            "Single-patient-use pen that delivers 2 mg per injection (3)\n",
            "Personal or family history of MTC or in patients with MEN 2 (4).\n",
            "Serious hypersensitivity reaction to semaglutide or any of the excipients in OZEMPIC (4).\n",
            "Acute Pancreatitis: Has been observed in patients treated with GLP-1 receptor agonists, including\n",
            "semaglutide. Discontinue if pancreatitis is suspected. (5.2).\n",
            "Diabetic Retinopathy Complications: Has been reported in a clinical trial. Patients with a history of\n",
            "diabetic retinopathy should be monitored (5.3).\n",
            "Never share an OZEMPIC pen between patients, even if the needle is changed (5.4).\n",
            "Hypoglycemia: Concomitant use with an insulin secretagogue or insulin may increase the risk of\n",
            "Metadata: {'page_number': 2}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are optimising for retrieval - i.e. how fine-grained is the context that we can retrieve so that we can answer the question that our user is asking. In this case, it might be better to have a smaller chunk because you are then narrowing down on the perfect part of the text where this information is present."
      ],
      "metadata": {
        "id": "7tqcP7ljBQsK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "from google.colab import userdata\n",
        "\n",
        "## Define the embedding model below by using the `text-embedding-3-small` model\n",
        "\n",
        "embedding_model = OpenAIEmbeddings(model='text-embedding-3-small', openai_api_key=userdata.get('OPENAI_API_KEY'))"
      ],
      "metadata": {
        "id": "UBVkp6TbaZDr"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A huge advantage of using Langchain is that it provides integrations with several types of vector databases like Chroma, Pinecone and more. In this project, we make use of the FAISS library from Facebook/Meta as a simple choice. In the following section, we combine the steps of generating the embedding value for each document chunk and then also storing it into the FAISS vector database."
      ],
      "metadata": {
        "id": "IDaUK7NbE8J4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "vector_store = FAISS.from_documents(documents=split_docs, embedding=embedding_model)"
      ],
      "metadata": {
        "id": "NDV_HnMFFJfb"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have our documents indexed and we can directly start retrieving documents based on a similarity search with a target query."
      ],
      "metadata": {
        "id": "_nI7cwlNdiBr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "search_result = vector_store.similarity_search_with_relevance_scores(query=\"What is the recommended dosage?\", k=4)\n",
        "search_result"
      ],
      "metadata": {
        "id": "x1_wRhCQJBLJ",
        "outputId": "947193ef-2221-42ae-b4ba-a489fbfad290",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(Document(id='a8da6fd2-0307-49d7-adcd-b9b66d670a31', metadata={'page_number': 1}, page_content='established cardiovascular disease (1).\\nto reduce the risk of sustained eGFR decline, end-stage kidney disease and cardiovascular death in\\nadults with type 2 diabetes mellitus and chronic kidney disease (1).\\nAdminister once weekly at any time of day, with or without meals (2.1).\\nStart at 0.25 mg once weekly. After 4 weeks, increase the dosage to 0.5 mg once weekly (2.2).\\nIf additional glycemic control is needed, increase the dosage to 1 mg once weekly after at least 4\\nweeks on the 0.5 mg dose (2.2).\\nIf additional glycemic control is needed, increase the dosage to 2 mg once weekly after at least 4\\nweeks on the 1 mg dosage (2.2).\\nTo reduce the risk of sustained eGFR decline, end-stage kidney disease and cardiovascular death,\\nincrease the dosage to 1 mg once weekly after at least 4 weeks on the 0.5 mg dosage (1, 2.2).\\nIf a dose is missed, administer within 5 days of missed dose (2.1).\\nInject subcutaneously in the abdomen, thigh, or upper arm (2.1).\\nSingle-patient-use pen that delivers 0.25 mg or 0.5 mg per injection (3)'),\n",
              "  np.float32(0.27134657)),\n",
              " (Document(id='1cda22fe-673c-4552-bb5d-89b1cc28858b', metadata={'page_number': 5}, page_content='‚Ä¢\\n‚Ä¢\\n2.2 Recommended Dosage\\nRecommended Initiation Dosage\\nInitiate OZEMPIC with a dosage of 0.25 mg injected subcutaneously once weekly for 4\\nweeks. Follow the dosage escalation below to reduce the risk of gastrointestinal adverse\\nreactions [see Warnings and Precautions (5.7), Adverse Reactions (6.1)].\\nAfter 4 weeks on the 0.25 mg dosage, increase the dosage to 0.5 mg once weekly.\\nRecommended Maintenance and Maximum Dosages for Glycemic Control\\nThe recommended maintenance dosage is 0.5 mg, 1 mg, or 2 mg, injected\\nsubcutaneously once weekly, based on glycemic control.\\nIf additional glycemic control is needed after at least 4 weeks on the:\\n‚Ä¢\\n‚Ä¢\\nThe maximum recommended dosage is 2 mg once weekly.\\nRecommended Maintenance Dosage in Patients with Type 2 Diabetes Mellitus and\\nChronic Kidney Disease\\nIncrease the dosage to the maintenance dosage, 1 mg once weekly, after at least 4\\nweeks on the 0.5 mg dosage.\\n3 DOSAGE FORMS AND STRENGTHS\\nInjection: clear, colorless solution available in 3 prefilled, disposable, single-patient-use\\npens:\\nDose per Injection\\nTotal Strength per Total\\nVolume\\nStrength per mL\\n0.25 mg\\n0.5 mg\\n2 mg / 3 mL\\n0.68 mg/mL\\n1 mg\\n4 mg / 3 mL\\n1.34 mg/mL\\n2 mg\\n8 mg / 3 mL\\n2.68 mg/mL\\nThe 2 mg/1.5 mL (1.34 mg/mL) strength is not currently marketed by Novo Nordisk Inc.\\n4 CONTRAINDICATIONS\\nother.\\nThe day of weekly administration can be changed if necessary as long as the time\\nbetween two doses is at least 2 days (>48 hours).\\nIf a dose is missed, administer OZEMPIC as soon as possible within 5 days after the\\nmissed dose. If more than 5 days have passed, skip the missed dose and\\nadminister the next dose on the regularly scheduled day. In each case, patients can\\nthen resume their regular once weekly dosing schedule.\\n0.5 mg dosage, the dosage may be increased to 1 mg once weekly.\\n1 mg dosage, the dosage may be increased to 2 mg once weekly.'),\n",
              "  np.float32(0.26324493)),\n",
              " (Document(id='7059af51-7de4-463c-8ef9-f6c614ea5ffd', metadata={'page_number': 75}, page_content='PRINCIPAL DISPLAY PANEL ‚Äì 1 mg dose, 4 mg/3 mL\\n1 mg\\nNDC 0169-4130-13List 413013'),\n",
              "  np.float32(0.20872736)),\n",
              " (Document(id='5acb96ef-763e-4c75-931a-f45d77fc7322', metadata={'page_number': 77}, page_content='PRINCIPAL DISPLAY PANEL ‚Äì 2 mg dose, 8mg/3 mL\\n2 mg\\nNDC 0169-4772-12List 477212'),\n",
              "  np.float32(0.19728053))]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this case, we have chosen to use cosine similarity as our distance metric and the similarity values are also shown along with the retrieved results. You will notice that not many of the retrieved chunks actually contain the text \"Weight Loss\" - and this is because we are not doing a word-based search but rather semantic search. This is what vector databases allow us to do. However, it is also possible that at times we want to match exact query terms and then we will follow a hybrid search approach. This is again one of the levers that we have to expriment with to determine what is necessary for our product. So it's important that you are able to determine what is the best method for you to choose - depends a lot on your use-case."
      ],
      "metadata": {
        "id": "SxPewWEYLG1R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating the QnA RAG chain"
      ],
      "metadata": {
        "id": "H_MbjNwSaEOn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's move on to complete our product by integrating with the LLM. Here again, we rely heavily on the building blocks that Langchain already provides us to put together the entire chain.\n",
        "\n",
        "The chain consists of multiple parts - a Retriever, followed by a Prompt Template where the retrieved documents are added and then the call to the LLM. The response from the LLM is what we finally show as output to the user. There are multiple steps in creating a RAG application. We break it down to make it easier to understand and follow."
      ],
      "metadata": {
        "id": "i7eiBrgeetbs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìù Learner Task:\n",
        "Creating the Prompt Template. In the section below, please create the PromptTemplate that will be used for your RAG application. The `ChatPromptTemplate` has become the standardized way to use chat-based LLMs and consists of a `SystemMessage`, followed by the `HumanMessage` with the response from the LLM stored in the `AIMessage`. We have provided the necessary imports in the below cell and request you to create the variable `qna_prompt_template` that will be used in the application."
      ],
      "metadata": {
        "id": "HlqW4TbcOfBw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain.prompts import SystemMessagePromptTemplate\n",
        "from langchain.prompts import HumanMessagePromptTemplate\n",
        "from langchain.prompts import AIMessagePromptTemplate\n",
        "\n",
        "## Your code here\n",
        "# Define the system message\n",
        "system_message = SystemMessagePromptTemplate.from_template(\n",
        "    \"You are a knowledgeable assistant that provides clear and concise answers.\"\n",
        ")\n",
        "\n",
        "# Define the human message\n",
        "human_message = HumanMessagePromptTemplate.from_template(\n",
        "    \"Question: {question}\"\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "# Combine into a chat prompt template\n",
        "qna_prompt_template = ChatPromptTemplate.from_messages([\n",
        "    system_message,\n",
        "    human_message\n",
        "])\n",
        "\n",
        "# Example usage\n",
        "print(qna_prompt_template.format_messages(question=\"What is the capital of France?\", answer=\"Paris\"))\n"
      ],
      "metadata": {
        "id": "5cv_Fi0IPfn5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ddd9a8db-c88b-45fa-e261-2258722b0aa8"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SystemMessage(content='You are a knowledgeable assistant that provides clear and concise answers.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Question: What is the capital of France?', additional_kwargs={}, response_metadata={})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After the PromptTemplate, the next step is to define the LLM Model that you would like to run with. We provide the OpenAI API models as the default option and make use of `gpt-4o-mini`. We also set the `temperature` value to 0 as we do not want the model to be creative but rather answer based on the retrieved context from the PDF."
      ],
      "metadata": {
        "id": "iumOVFV4Pk_X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(model_name=\"gpt-4o-mini\",\n",
        "                 temperature=0,\n",
        "                 openai_api_key=userdata.get('OPENAI_API_KEY'))"
      ],
      "metadata": {
        "id": "9-wDIb1gQI7E"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next step is to initilize the retriever part of our application that will fetch the relevant documents from the PDF. Recall that we have already created the embeddings for our document chunks and stored it using the FAISS vectorstore. In this step, we only specify that vectorestore to be our retriever."
      ],
      "metadata": {
        "id": "nDlnYaodQPeU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = vector_store.as_retriever()"
      ],
      "metadata": {
        "id": "hz_SleAiQrTi"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "One additional step that we need to also consider is that when the documents are retrieved from the FAISS vectorstore, they are in the form of a list of documents. These document objects contain a lot more information other than the content like Metadata. Assuming we are performing a simple retrieval, the context that we want to pass to the LLM is only the text content. Therefore we write an additional function that combines only the retrieved text content that can be used to pass in the context of our PromptTemplate.\n",
        "\n",
        "Please note that you could also choose to filter the retrieved documents based on the Metadata. For instance, if you only want to see results from Page 5 and beyond of the PDF - then you can also adapt this function to reflect that."
      ],
      "metadata": {
        "id": "zEtojMFZQyjR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
      ],
      "metadata": {
        "id": "b64wwSooQv2f"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The final stage is to put together the various elements that we have defined above to create our pipeline. The key aspect to note here is that the input question is used twice during the process -> at the first instance to retrieve a list of the relevant context and the second instance when it is passed to the LLM as the question that needs to be answered.\n",
        "\n",
        "So we make use of the `itemgetter` to retrieve the content of the question and pass to the retriever to get the relevant context. On the other hand, we make use of the `RunnablePassthrough` class to directly pass the input question to the subsequent operation. At the end we also add the `StrOuputParser` to get the actual text message of the response rather than the whole AIMessage."
      ],
      "metadata": {
        "id": "0VVUVqJARaYJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from operator import itemgetter\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "rag_chain = (\n",
        "    {\"context\": itemgetter(\"input\") | retriever | format_docs,\n",
        "     \"question\": RunnablePassthrough()}\n",
        "    | qna_prompt_template\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")"
      ],
      "metadata": {
        "id": "VI7UOE70LleJ"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please check that your entire chain works by providing your questions and invoking the RAG chain."
      ],
      "metadata": {
        "id": "t5YW6DF-Sog3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = 'What is the recommended dosage of ozempic?'\n",
        "rag_chain.invoke({\"input\": question})"
      ],
      "metadata": {
        "id": "ftZIkyQISoAt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "f4520a57-07ec-4efc-d17c-65ee5468256b"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"The recommended starting dosage of Ozempic (semaglutide) for adults with type 2 diabetes is typically 0.25 mg once a week for the first four weeks. After that, the dosage can be increased to 0.5 mg once a week. If additional glycemic control is needed, the dose may be further increased to 1 mg once a week after at least four weeks on the 0.5 mg dose. It's important to follow your healthcare provider's instructions regarding dosage and to discuss any concerns or side effects.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìù Learner Task:\n",
        "If you run into errors as you develop, visualizing the chain is an easy way to discover any issues. This visualization helps us understand the entire flow of operations within our application. By graphically representing the connections and interactions between different components we can see how the data flows and in which shape from one path tothe next. It allows for easier debugging and we might be able to pinpoint where modifications might be necessary."
      ],
      "metadata": {
        "id": "PBbDadkAT1zB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print (rag_chain.get_graph().print_ascii())"
      ],
      "metadata": {
        "id": "l50o8HECL53q",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc6b5584-895b-4aa0-ee51-0e37d428b2b9"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "           +---------------------------------+        \n",
            "           | Parallel<context,question>Input |        \n",
            "           +---------------------------------+        \n",
            "                    **              ***               \n",
            "                 ***                   **             \n",
            "               **                        ***          \n",
            "       +--------+                           **        \n",
            "       | Lambda |                            *        \n",
            "       +--------+                            *        \n",
            "            *                                *        \n",
            "            *                                *        \n",
            "            *                                *        \n",
            "+----------------------+                     *        \n",
            "| VectorStoreRetriever |                     *        \n",
            "+----------------------+                     *        \n",
            "            *                                *        \n",
            "            *                                *        \n",
            "            *                                *        \n",
            "    +-------------+                   +-------------+ \n",
            "    | format_docs |                   | Passthrough | \n",
            "    +-------------+*                  +-------------+ \n",
            "                    **              **                \n",
            "                      ***        ***                  \n",
            "                         **    **                     \n",
            "          +----------------------------------+        \n",
            "          | Parallel<context,question>Output |        \n",
            "          +----------------------------------+        \n",
            "                            *                         \n",
            "                            *                         \n",
            "                            *                         \n",
            "                  +--------------------+              \n",
            "                  | ChatPromptTemplate |              \n",
            "                  +--------------------+              \n",
            "                            *                         \n",
            "                            *                         \n",
            "                            *                         \n",
            "                      +------------+                  \n",
            "                      | ChatOpenAI |                  \n",
            "                      +------------+                  \n",
            "                            *                         \n",
            "                            *                         \n",
            "                            *                         \n",
            "                   +-----------------+                \n",
            "                   | StrOutputParser |                \n",
            "                   +-----------------+                \n",
            "                            *                         \n",
            "                            *                         \n",
            "                            *                         \n",
            "                +-----------------------+             \n",
            "                | StrOutputParserOutput |             \n",
            "                +-----------------------+             \n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Hopefully, you have managed to build the skeleton for your product. At first just make sure that everything is working in an end to end fashion. After you have achieved that you can move in the direction of improving the answers from the application. This is when you can start experimenting with different aspects of your pipeline to go in the direction of a better solution.\n",
        "\n",
        "We would recommend adapting the following options:\n",
        "\n",
        "- Chunk size:\n",
        "- Retrieval strategy/ metric:\n",
        "- Choice of LLM:"
      ],
      "metadata": {
        "id": "VwTao9CgfNrN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "At the end of this stage, you should be able to have a working prototype of your solution that does reasonably well for the use-case you have in mind.\n",
        "\n",
        "We are still relying on a vibe/gut-feel for the product and cannot rely on quantified metrics. But hopefully you can feel reasonably confident to launch an alpha version of your product to a select group of customers."
      ],
      "metadata": {
        "id": "IvhtzIRbfrec"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Deploying the PillPal bot"
      ],
      "metadata": {
        "id": "NlvAHj4bPl6Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before deploying our product, one of the important steps is to enable tracking and monitoring all the user queries and LLM responses. This is made very easy for us with the help of Langsmith - another part of the Langchain ecosystem.\n",
        "\n",
        "This is one of the cornerstones of our strategy to get towards a more valuable and performing LLM app.\n",
        "\n",
        "Please sign-up on the Langsmith [Website](https://smith.langchain.com/) and you should have access. Next, please navigate to the Settings page from the left side navigation menu and then create your API key there and copy it.\n",
        "\n",
        "<a href=\"https://ibb.co/fN8s3Yp\"><img src=\"https://i.ibb.co/ZgH0Q68/Screenshot-2024-07-26-at-16-12-48.png\" alt=\"Screenshot-2024-07-26-at-16-12-48\" border=\"0\"></a>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "After that, you can return to the main page and click the New Project icon and copy the environment details that you see.\n",
        "\n",
        "<a href=\"https://ibb.co/th2CQhd\"><img src=\"https://i.ibb.co/yRksdR3/Screenshot-2024-07-26-at-16-12-59.png\" alt=\"Screenshot-2024-07-26-at-16-12-59\" border=\"0\"></a>"
      ],
      "metadata": {
        "id": "0X3ZhG3_PqrI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['LANGSMITH_TRACING'] = 'true'\n",
        "os.environ['LANGSMITH_ENDPOINT'] = \"https://api.smith.langchain.com\"\n",
        "os.environ['LANGSMITH_API_KEY'] = userdata.get('LANGSMITH_API_KEY')\n",
        "os.environ['LANGSMITH_PROJECT'] = \"YOUR_PROJECT_NAME\""
      ],
      "metadata": {
        "id": "WeuJlFzdOnBA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can deploy our LLM app in two different ways -\n",
        "\n",
        "1. A website based chatbot experience - this will be done with the help of Gradio and you do not need to leave the Colab notebook environment\n",
        "2. A telegram chatbot that will run on Github Codespaces (or locally) and will need you to leave the Colab environment"
      ],
      "metadata": {
        "id": "-AylFlTLPyp5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Deployed as website chatbot\n",
        "\n",
        "We can deploy our QnA bot on a dedicated website using Gradio, an easy-to-use library for creating interactive machine learning interfaces. One significant advantage of using Gradio in our project is that it integrates seamlessly within the Colab notebook environment. This means you don't need to leave Colab to see your bot in action; you can run it directly from the notebook. Gradio also provides a dedicated URL that you can share with potential beta testers of the QnA bot to get feedback."
      ],
      "metadata": {
        "id": "4TQ_kDighGgL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.schema import AIMessage, HumanMessage\n",
        "import openai\n",
        "import gradio as gr\n",
        "\n",
        "def predict(message, history):\n",
        "    return rag_chain.invoke({\"input\":message})\n",
        "\n",
        "gr.ChatInterface(predict).launch()"
      ],
      "metadata": {
        "id": "DDy0V7_eoEYt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### [OPTIONAL] Deployed as Telegram bot\n",
        "\n",
        "For a wider launch of your LLM app, you can easily make it available as a bot on Telegram - a messenger platform that is used by 500 million users every day. This is useful in the beta/alpha phase because it's way easier to get users to try it in an app that they already use every day. This need not be just Telegram but other popular messengers like Whatsapp, Signal etc. We chose Telegram as it's free and easy to setup.\n",
        "\n",
        "In order to deploy using this option, we will make use of Github codespaces as it's difficult to run it within a synchronous coding environment like Colab. The instructions and code for this will be provided in the live project session."
      ],
      "metadata": {
        "id": "_6TnwMSgpNps"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please go to the Github repository: https://github.com/sidhusmart/uplimit_langchain.git and clone it so that you can work on it directly. Please note that code in this repository may change frequently so we request you to work only on your cloned version.\n",
        "\n",
        "Once you have cloned it, then you can launch your own Codespaces instance by following the steps as shown in the screenshot. This creates a virtual Visual Studio Code environment in the cloud where we can test our Telegram bot.\n",
        "\n",
        "<a href=\"https://ibb.co/p02fJ1t\"><img src=\"https://i.ibb.co/BzLyqfX/Screenshot-2024-07-26-at-16-05-46.png\" alt=\"Screenshot-2024-07-26-at-16-05-46\" border=\"0\"></a>"
      ],
      "metadata": {
        "id": "mmX9Z7lZ8eWT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The two important files for you to take a look are `langchain_rag_app.py` and `telegram_response_bot.py` The first python file contains the same RAG functionality that we already built but packaged in functions. The telegram response bot contains the necessary code required to run the Telegram bot.\n",
        "\n",
        "In order to setup and run this app, we have to add the necessary API keys. This is the same process as what we have done previously in the Colab notebook but since we are executing this code in a new environment, we have to add the necessary secrets there as well.\n",
        "\n",
        "We do this via the `.env.dev` file. This file does not exist in the repository by default but instead we have provided the template file called `.env.template`. You need to create a copy of this file and rename it to be `.env.dev` that will contain all the API keys you are going to use during development of the app. Please note that this file is not checked in as we do not want to commit the secrets!  \n",
        "\n",
        "You can observe that there are several environment variables defined in the .env file - most of which we have already setup. You can use the same values for those keys again.\n",
        "\n",
        "However, there is one additional API key that is required to work with the Telegram API. In order to get the Telegram bot API key, you need to follow these steps:\n",
        "\n",
        "1. You must have a Telegram account and access to the App - Web or Mobile versions would also work.\n",
        "2. On the Telegram app, you need to search and look for a contact called the Botfather.\n",
        "3. You can use the `/start` command to get a list of options or directly use the `/newbot` command to create a new bot for yourself.\n",
        "4. Then answer the set of questions by providing a name for and username for your bot.\n",
        "5. At the end you will be provided with an API token that you can use to work with the bot. This API token is what we need to replace in the .env.dev file. `telegram_response_bot.py` file.\n",
        "\n",
        "<a href=\"https://imgbb.com/\"><img src=\"https://i.ibb.co/f2MqxCd/Screenshot-2024-07-26-at-14-23-02.png\" alt=\"Screenshot-2024-07-26-at-14-23-02\" border=\"0\"></a>"
      ],
      "metadata": {
        "id": "zC344Wj19pcE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once you have performed both these steps, you are now ready to run your RAG app as a Telegam bot. You need to run the command `python telegram_response_bot.py ` from the command line of your Github Space and this starts a thread that listens for messages on your bot. Once a message is received, it then hands off to the RAG app and provides the generated response.\n",
        "\n",
        "You can easily share this Pillpal bot with your friends and family and ask them to try it!"
      ],
      "metadata": {
        "id": "aOutTlUR_zmC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Evaluation of PillPal"
      ],
      "metadata": {
        "id": "Y_4zSQWPQM-R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have deployed and made our app live, we need to consistently monitor how it performs. This is also a way for us to understand what questions are being asked by our users and whether the bot is responding correctly or not. For this we will make use of the Langsmith part of the Langchain library."
      ],
      "metadata": {
        "id": "m9wzLbcQptfZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As the first step, we have to create a validation dataset that we can use for evaluation. There are several ways to go about doing this:\n",
        "\n",
        "1. Launch your app in beta and get some trial users to start interacting with the bot/chat interface and identify common patterns and questions that are being asked. Manually, write your own answers to these questions and perform evaluations using that.\n",
        "2. Come up with a list of 5-10 questions and manually search for the answers of these questions in the PDF and create this as your evaluation dataset -> this is what we are going to do below.  \n",
        "3. You could also make use of another LLM to come up with these questions and answers which is called creating a synthetic dataset. This would work well if you use higher level models like GPT-4 but you need to keep in mind that they have to be relevant to the context of your leaflet."
      ],
      "metadata": {
        "id": "Gvj_e6UYon-V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating a base evaluation dataset"
      ],
      "metadata": {
        "id": "b-H03qVVaMkH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To ensure that our application is functioning correctly and effectively answering questions, we have implemented a basic sanity check. This involves a set of 8-10 questions that I have manually curated and answered after thoroughly reviewing the content of the PDF document. These questions are designed to cover a wide range of topics and complexities within the document, ensuring that they test various aspects of our QnA bot's capabilities. By comparing the answers generated by our bot to these manually prepared responses, we can gauge the accuracy and reliability of our system. This step is crucial as it helps us identify any discrepancies or areas needing improvement before the application is deployed for wider use. Additionally, it provides an initial layer of validation and builds confidence in the application's performance among users and stakeholders."
      ],
      "metadata": {
        "id": "6xTfANXXXd5P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìù Learner Task:\n",
        "\n",
        "Please create a set of question and correct answer pairs here to use as your evaluation dataset. We have provided the first two questions as examples but feel free to overwrite and add your own."
      ],
      "metadata": {
        "id": "Vv6vi_EAT89n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "examples = [\n",
        "    (\n",
        "        \"Can it be taken by pregnant women?\",\n",
        "        \"No, it should not be taken by pregnant women before seeking the doctors advice.\",\n",
        "    ),\n",
        "    (\n",
        "        \"Can it be given to children?\",\n",
        "        \"No, it should be taken by children aged less than 6 years old. Children above the age of 6 must follow the prescribed dosage - half a tablet twice daily for children between 6 and 12 years.\",\n",
        "    )\n",
        "]"
      ],
      "metadata": {
        "id": "wXFxOClWQPbe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once we have successfully created our dataset, the next step involves registering it with the Langsmith client. This process integrates the dataset into our evaluation framework. By registering the dataset, we effectively add it to the list of datasets available for testing and validating our model. In general, you might create multiple datasets to test various aspects of the QnA bot and might run different types of evaluations on it.\n",
        "\n",
        "In this case, we are running a test for general correctness of the QnA bot - while this does include checks for hallucinations -> we have added examples where the bot should not have an answer and must say so. But there are additional datasets that you might create with separate evaluation metrics to explicitly check for hallucination and fact-checking."
      ],
      "metadata": {
        "id": "x82VVhfGqJ9S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langsmith import Client\n",
        "\n",
        "client = Client()"
      ],
      "metadata": {
        "id": "hGqC94J7qSEn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langsmith import Client\n",
        "\n",
        "client = Client()\n",
        "\n",
        "dataset_name = \"PillPall Accuracy Test Dataset\"\n",
        "dataset = client.create_dataset(dataset_name=dataset_name)\n",
        "inputs, outputs = zip(\n",
        "    *[({\"input\": input}, {\"expected\": expected}) for input, expected in examples]\n",
        ")\n",
        "client.create_examples(inputs=inputs, outputs=outputs, dataset_id=dataset.id)"
      ],
      "metadata": {
        "id": "2EbuuZNMqTRk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once the above cells are run, you should now be able to see the dataset created in the dataset folder in your Langsmith project.\n",
        "\n",
        "![QA Dataset](https://i.ibb.co/sH5dv86/Xnapper-2024-06-11-20-58-53.png)"
      ],
      "metadata": {
        "id": "EqUUjzJUq_F5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's run the evaluation of our app using this dataset.\n",
        "\n",
        "By executing the code below, we will first be calling our app for each of the questions in our QA dataset. Once the answers have been generated, we need to evaluate it with the golden standard answers that we have written manually. But the big question is always: how can we compare two texts, what metrics shall we use?\n",
        "\n",
        "One of the easier ways to do this is by using another LLM to compare the two responses and tell you whether they match or not. This is also referred to as 'LLM-as-a-judge'. In this case, since we have only 5-10 examples you can easily do this manually but typically your evaluation datasets will contain a lot of examples and this method would not work.\n",
        "\n",
        "We will use the off-the-shelf evaluator that Langsmith provides us called `cot_qa` that stands for Chain of Thought Question Answer. This basically refers to a pre-filled prompt that is used when asking the \"Judge LLM\" to rate whether the two answers are comparable or not.\n",
        "\n",
        "Typically we make use of a bigger, more powerful LLM to act as the judge for what the smaller, cheaper LLM has generated. However, since our dataset is small and quite easy we stick with `gpt-4o-mini`"
      ],
      "metadata": {
        "id": "kcZrdxNVrci2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langsmith import Client\n",
        "from langsmith.evaluation import LangChainStringEvaluator, evaluate\n",
        "\n",
        "eval_llm = ChatOpenAI(temperature=0.0, model=\"gpt-4o-mini\", openai_api_key=userdata.get('OPENAI_API_KEY'))\n",
        "cot_qa_evaluator = LangChainStringEvaluator(\"cot_qa\", config={\"llm\": eval_llm})\n",
        "\n",
        "client = Client()\n",
        "evaluate(\n",
        "    rag_chain,\n",
        "    data=dataset_name,\n",
        "    evaluators=[cot_qa_evaluator],\n",
        ")"
      ],
      "metadata": {
        "id": "vMe1SzrvrKMs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's run the evaluation on the dataset. You get the links to where you can track the run and at the end the output is also printed here."
      ],
      "metadata": {
        "id": "nd_6YQfQsvgw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After initiating the evaluation of our QnA bot using the dataset we registered, you will be able to observe several key performance metrics. These metrics include the percentage of questions answered correctly, which provides a direct measure of the bot's accuracy. You will also see the duration of the test, which gives insight into the efficiency of the bot under testing conditions. Additionally, the evaluation will report on costs and latency, offering a broader view of the bot's operational performance.\n",
        "\n",
        "This initial evaluation serves as a preliminary indication of how well your bot is performing. As the bot encounters more users and a diverse array of questions, it's crucial to continually update and adapt your baseline test dataset. This iterative process ensures that the bot remains effective and responsive to the evolving needs and contexts it will encounter in real-world applications.\n",
        "\n",
        "![Evaluation Test results](https://i.ibb.co/txtzhtD/Xnapper-2024-06-16-11-32-28.png)"
      ],
      "metadata": {
        "id": "C5PPIGVIYU8Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìù Learner Task:\n",
        "\n",
        "Now that you have an end to end RAG chatbot up and running, you can try to stress-test your bot by asking all kinds of questions - both relevant and irrelevant. What kind of responses does it generate? How often does it hallucinate?"
      ],
      "metadata": {
        "id": "Bj_PnIs8aouf"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CgQ9dSJJa61b"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}